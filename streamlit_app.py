# ========================================
# Streamlit AI í•™ìŠµ ì½”ì¹˜ (ë‹¤êµ­ì–´ ì§€ì› ì¶”ê°€)
# ========================================
import streamlit as st
import os
import tempfile
import time
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# ================================
# 0. ë‹¤êµ­ì–´ ì§€ì› ë”•ì…”ë„ˆë¦¬ (Language Dictionary)
# ================================
LANG = {
    "ko": {
        "title": "ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜",
        "sidebar_title": "ğŸ“š AI Study Coach ì„¤ì •",
        "file_uploader": "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        "button_start_analysis": "ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)",
        "rag_tab": "RAG ì§€ì‹ ì±—ë´‡",
        "content_tab": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "lstm_tab": "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "rag_header": "RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)",
        "rag_desc": "ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.",
        "rag_input_placeholder": "í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”",
        "llm_error_key": "âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.",
        "llm_error_init": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.",
        "content_header": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "content_desc": "í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±",
        "topic_label": "í•™ìŠµ ì£¼ì œ",
        "level_label": "ë‚œì´ë„",
        "content_type_label": "ì½˜í…ì¸  í˜•ì‹",
        "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
        "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 3ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
        "button_generate": "ì½˜í…ì¸  ìƒì„±",
        "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.",
        "lstm_header": "LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "lstm_desc": "ê°€ìƒì˜ ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ë˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.",
        "lstm_disabled_error": "í˜„ì¬ ë¹Œë“œ í™˜ê²½ ë¬¸ì œë¡œ ì¸í•´ LSTM ê¸°ëŠ¥ì€ ì ì •ì ìœ¼ë¡œ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. 'ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±' ê¸°ëŠ¥ì„ ë¨¼ì € ì‚¬ìš©í•´ ì£¼ì„¸ìš”.",
        "lang_select": "ì–¸ì–´ ì„ íƒ"
    },
    "en": {
        "title": "Personalized AI Study Coach",
        "sidebar_title": "ğŸ“š AI Study Coach Settings",
        "file_uploader": "Upload Study Materials (PDF, TXT, HTML)",
        "button_start_analysis": "Start Analysis (RAG Indexing)",
        "rag_tab": "RAG Knowledge Chatbot",
        "content_tab": "Custom Content Generation",
        "lstm_tab": "LSTM Achievement Prediction",
        "rag_header": "RAG Knowledge Chatbot (Document Q&A)",
        "rag_desc": "Answers questions based on the uploaded documents.",
        "rag_input_placeholder": "Ask a question about your study materials",
        "llm_error_key": "âš ï¸ Warning: GEMINI API Key is not set. Please set 'GEMINI_API_KEY' in Streamlit Secrets.",
        "llm_error_init": "LLM initialization error: Please check your API key.",
        "content_header": "Custom Learning Content Generation",
        "content_desc": "Generate content tailored to your topic and difficulty.",
        "topic_label": "Learning Topic",
        "level_label": "Difficulty",
        "content_type_label": "Content Type",
        "level_options": ["Beginner", "Intermediate", "Advanced"],
        "content_options": ["Key Summary Note", "3 Multiple-Choice Questions", "Practical Example Idea"],
        "button_generate": "Generate Content",
        "warning_topic": "Please enter a learning topic.",
        "lstm_header": "LSTM Based Achievement Prediction",
        "lstm_desc": "Trains an LSTM model on hypothetical past quiz scores to predict future achievement.",
        "lstm_disabled_error": "The LSTM feature is temporarily disabled due to build environment issues. Please use the 'Custom Content Generation' feature first.",
        "lang_select": "Select Language"
    }
}
if 'language' not in st.session_state:
    st.session_state.language = 'ko'

# í˜„ì¬ ì–¸ì–´ ë”•ì…”ë„ˆë¦¬ ë¡œë“œ
L = LANG[st.session_state.language]

# ================================
# 1. LLM ë° ì„ë² ë”© ì´ˆê¸°í™” + ì„ë² ë”© ìºì‹œ
# (ì´ì „ ì½”ë“œì™€ ë™ì¼)
# ================================
API_KEY = os.environ.get("GEMINI_API_KEY")

if 'llm' not in st.session_state:
    if not API_KEY:
        st.error(L["llm_error_key"])
        st.session_state.is_llm_ready = False
    else:
        try:
            st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, google_api_key=API_KEY)
            st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=API_KEY)
            st.session_state.is_llm_ready = True
        except Exception as e:
            st.error(f"{L['llm_error_init']} {e}")
            st.session_state.is_llm_ready = False

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

if "embedding_cache" not in st.session_state:
    st.session_state.embedding_cache = {}

# ================================
# 2. LSTM ëª¨ë¸ ì •ì˜ (ë³µêµ¬ëœ ì˜ì—­)
# (ì´ì „ ì½”ë“œì™€ ë™ì¼)
# ================================
@st.cache_resource
def load_or_train_lstm():
    # ... (LSTM ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ ë¡œì§ì€ ì´ì „ ì½”ë“œì™€ ë™ì¼)
    # [Code Snippet for load_or_train_lstm() function]
    """ê°€ìƒì˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ì„ ìœ„í•œ LSTM ëª¨ë¸ì„ ìƒì„±í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤."""
    # 1. ê°€ìƒ ë°ì´í„° ìƒì„±: 10ì£¼ê°„ì˜ í€´ì¦ˆ ì ìˆ˜ (0-100)
    np.random.seed(42)
    data = np.cumsum(np.random.normal(loc=5, scale=5, size=50)) + 60
    data = np.clip(data, 50, 95)  # ì ìˆ˜ ë²”ìœ„ ì œí•œ

    # 2. ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬
    def create_dataset(dataset, look_back=3):
        X, Y = [], []
        for i in range(len(dataset) - look_back):
            X.append(dataset[i:(i + look_back)])
            Y.append(dataset[i + look_back])
        return np.array(X), np.array(Y)

    look_back = 5
    X, Y = create_dataset(data, look_back)

    # LSTM ì…ë ¥ í˜•íƒœ ë§ì¶”ê¸°: [samples, time steps, features]
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    # 3. LSTM ëª¨ë¸ ì •ì˜
    model = Sequential([
        LSTM(50, activation='relu', input_shape=(look_back, 1)),
        Dense(1)
    ])

    # 4. ëª¨ë¸ í•™ìŠµ (ë¹ ë¥¸ ì‹œì—°ì„ ìœ„í•´ ìµœì†Œí•œì˜ epochë§Œ ì„¤ì •)
    model.compile(optimizer='adam', loss='mse')
    model.fit(X, Y, epochs=10, batch_size=1, verbose=0)

    return model, data

# --- RAG ê´€ë ¨ í•¨ìˆ˜ (ì´ì „ ì½”ë“œì™€ ë™ì¼) ---
def get_document_chunks(files):
    # [Code Snippet for get_document_chunks() function]
    documents = []
    temp_dir = tempfile.mkdtemp()

    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        file_extension = uploaded_file.name.split('.')[-1].lower()
        
        if file_extension == "pdf":
            with open(temp_filepath, "wb") as f:
                f.write(uploaded_file.getvalue())
            loader = PyPDFLoader(temp_filepath)
            documents.extend(loader.load())
        
        elif file_extension == "html":
            raw_html = uploaded_file.getvalue().decode('utf-8')
            soup = BeautifulSoup(raw_html, 'html.parser')
            text_content = soup.get_text(separator=' ', strip=True)
            
            from langchain.schema.document import Document
            documents.append(Document(page_content=text_content, metadata={"source": uploaded_file.name}))


        elif file_extension == "txt":
            with open(temp_filepath, "wb") as f:
                f.write(uploaded_file.getvalue())
            loader = TextLoader(temp_filepath, encoding="utf-8")
            documents.extend(loader.load())
            
        else:
            st.warning(f"'{uploaded_file.name}' íŒŒì¼ì€ í˜„ì¬ PDF, TXT, HTMLë§Œ ì§€ì›í•˜ì—¬ ë¡œë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    return text_splitter.split_documents(documents)


def get_vector_store(text_chunks):
    # [Code Snippet for get_vector_store() function]
    cache_key = tuple(doc.page_content for doc in text_chunks)
    if cache_key in st.session_state.embedding_cache:
        st.info("âœ… ì„ë² ë”© ìºì‹œê°€ ë°œê²¬ë˜ì–´ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤. (API í•œë„ ì ˆì•½)")
        return st.session_state.embedding_cache[cache_key]
    
    if not st.session_state.is_llm_ready:
        return None

    try:
        vector_store = FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)
        st.session_state.embedding_cache[cache_key] = vector_store
        return vector_store
    
    except Exception as e:
        if "429" in str(e):
             st.error("âš ï¸ **API ì„ë² ë”© í•œë„ ì´ˆê³¼ (429 Error)**: Google Gemini APIì˜ ë¬´ë£Œ ì„ë² ë”© ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ API ì‚¬ìš©ëŸ‰ ëŒ€ì‹œë³´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            st.error(f"Vector Store ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def get_rag_chain(vector_store):
    # [Code Snippet for get_rag_chain() function]
    if vector_store is None:
        return None
        
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )


# ================================
# 4. Streamlit UI (â­ì œëª©ê³¼ ì‚¬ì´ë“œë°” ìˆ˜ì •â­)
# ================================
st.set_page_config(page_title=L["title"], layout="wide")

with st.sidebar:
    # ì–¸ì–´ ì„ íƒ ë“œë¡­ë‹¤ìš´ ì¶”ê°€
    st.session_state.language = st.selectbox(
        L["lang_select"],
        options=['ko', 'en'],
        format_func=lambda x: "í•œêµ­ì–´" if x == 'ko' else "English"
    )
    # ì–¸ì–´ ë³€ê²½ ì‹œ UI í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (L ì¬í• ë‹¹)
    L = LANG[st.session_state.language] 

    st.title(L["sidebar_title"])
    st.markdown("---")

    uploaded_files = st.file_uploader(
        L["file_uploader"],
        type=["pdf","txt","html"],
        accept_multiple_files=True
    )

    if uploaded_files and st.session_state.is_llm_ready:
        if st.button(L["button_start_analysis"], key="start_analysis"):
            with st.spinner(f"ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘..."):
                text_chunks = get_document_chunks(uploaded_files)
                vector_store = get_vector_store(text_chunks)
                
                if vector_store:
                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                    st.success(f"ì´ {len(text_chunks)}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!")
                else:
                    st.session_state.is_rag_ready = False
                    st.error("ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ.")

    else:
        st.session_state.is_rag_ready = False
        st.warning("ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    st.markdown("---")
    feature_selection = st.radio(
        L["content_tab"], # ë¼ë””ì˜¤ ë²„íŠ¼ ì œëª©ë„ ì–¸ì–´ì— ë§ê²Œ ë³€ê²½
        [L["rag_tab"], L["content_tab"], L["lstm_tab"]]
    )

st.title(L["title"])

# ================================
# 5. ê¸°ëŠ¥ë³„ í˜ì´ì§€ êµ¬í˜„ (â­í…ìŠ¤íŠ¸ ìš”ì†Œ ëª¨ë‘ L[]ë¡œ ë³€ê²½â­)
# ================================
if feature_selection == L["rag_tab"]:
    st.header(L["rag_header"])
    st.markdown(L["rag_desc"])
    if st.session_state.is_rag_ready and st.session_state.conversation_chain:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input(L["rag_input_placeholder"]):
            st.session_state.messages.append({"role":"user","content":prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            with st.chat_message("assistant"):
                with st.spinner(f"ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.conversation_chain.invoke({"question":prompt})
                        answer = response.get('answer','ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role":"assistant","content":answer})
                    except Exception as e:
                        st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}")
                        st.session_state.messages.append({"role":"assistant","content":"ì˜¤ë¥˜ ë°œìƒ"})
    else:
        st.warning(f"RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”.")

elif feature_selection == L["content_tab"]:
    st.header(L["content_header"])
    st.markdown(L["content_desc"])

    if st.session_state.is_llm_ready:
        topic = st.text_input(L["topic_label"])
        
        # ë‚œì´ë„ì™€ ì½˜í…ì¸  íƒ€ì…ì€ ë”•ì…”ë„ˆë¦¬ì—ì„œ ë¶ˆëŸ¬ì˜¨ ì˜µì…˜ìœ¼ë¡œ ë³€ê²½
        level = st.selectbox(L["level_label"], L["level_options"])
        content_type = st.selectbox(L["content_type_label"], L["content_options"])

        if st.button(L["button_generate"]):
            if topic:
                # LLM í”„ë¡¬í”„íŠ¸ ìƒì„± (ìš”ì²­ ì–¸ì–´ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ê°€)
                target_lang = "Korean" if st.session_state.language == 'ko' else "English"
                
                full_prompt = f"""You are a professional AI coach at the {level} level.
Please generate clear and educational content in the requested {content_type} format based on the topic.
The response MUST be strictly in {target_lang}.

Topic: {topic}
Requested Format: {content_type}"""

                with st.spinner(f"Generating {content_type} for {topic}..."):
                    try:
                        response = st.session_state.llm.invoke(full_prompt)
                        st.success(f"**{topic}** - **{content_type}** Result:")
                        st.markdown(response.content)
                    except Exception as e:
                        st.error(f"Content Generation Error: {e}")
            else:
                st.warning(L["warning_topic"])
    else:
        st.error(L["llm_error_init"])

elif feature_selection == L["lstm_tab"]:
    st.header(L["lstm_header"])
    st.markdown(L["lstm_desc"])

    with st.spinner("LSTM model loading/training..."):
        try:
            # 1. ëª¨ë¸ ë¡œë“œ ë° ë°ì´í„° ìƒì„±
            lstm_model, historical_scores = load_or_train_lstm()
            st.success("LSTM Model Ready!")

            # 2. ì˜ˆì¸¡ ë¡œì§
            look_back = 5
            last_sequence = historical_scores[-look_back:]
            input_sequence = np.reshape(last_sequence, (1, look_back, 1))
            
            future_predictions = []
            current_input = input_sequence

            for i in range(5):
                next_score = lstm_model.predict(current_input, verbose=0)[0]
                future_predictions.append(next_score[0])

                next_input = np.append(current_input[:, 1:, :], next_score[0]).reshape(1, look_back, 1)
                current_input = next_input

            # 3. ì‹œê°í™”
            fig, ax = plt.subplots(figsize=(10, 6))

            ax.plot(range(len(historical_scores)), historical_scores, label="Past Quiz Scores (Hypothetical)", marker='o', linestyle='-', color='blue')
            future_indices = range(len(historical_scores), len(historical_scores) + len(future_predictions))
            ax.plot(future_indices, future_predictions, label="Predicted Achievement (Next 5 Days)", marker='x', linestyle='--', color='red')

            ax.set_title(L["lstm_header"])
            ax.set_xlabel(L["topic_label"])
            ax.set_ylabel("Achievement Score (0-100)")
            ax.legend()
            ax.grid(True)
            st.pyplot(fig)

            # 4. LLM ë¶„ì„ ì½”ë©˜íŠ¸
            st.markdown("---")
            st.markdown("#### AI Coach Analysis Comment")
            # (ì´ ë¶€ë¶„ì€ í•œêµ­ì–´ë¡œ í†µì¼í•˜ì—¬ ê°„ê²°í•˜ê²Œ ìœ ì§€)
            avg_recent = np.mean(historical_scores[-5:])
            avg_predict = np.mean(future_predictions)
            
            # (ì´ ë¡œì§ì€ ì–¸ì–´ ë”•ì…”ë„ˆë¦¬ë¡œ ëŒ€ì²´í•˜ê¸°ê°€ ë³µì¡í•˜ì—¬ ì„ì‹œë¡œ ì˜ì–´/í•œêµ­ì–´ ë¶„ê¸°ë¡œ ì²˜ë¦¬)
            if st.session_state.language == 'ko':
                if avg_predict > avg_recent:
                    comment = "ìµœê·¼ í•™ìŠµ ë°ì´í„°ì™€ LSTM ì˜ˆì¸¡ ê²°ê³¼ì— ë”°ë¥´ë©´, **ì•ìœ¼ë¡œì˜ í•™ìŠµ ì„±ì·¨ë„ê°€ ê¸ì •ì ìœ¼ë¡œ í–¥ìƒë  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡**ë©ë‹ˆë‹¤. í˜„ì¬ í•™ìŠµ ë°©ì‹ì„ ìœ ì§€í•˜ì‹œê±°ë‚˜, ë‚œì´ë„ë¥¼ í•œ ë‹¨ê³„ ë†’ì—¬ ë„ì „í•´ ë³´ì„¸ìš”!"
                elif avg_predict < avg_recent - 5:
                    comment = "LSTM ì˜ˆì¸¡ ê²°ê³¼, **ì„±ì·¨ë„ê°€ ë‹¤ì†Œ í•˜ë½í•  ìˆ˜ ìˆë‹¤ëŠ” ì‹ í˜¸**ê°€ ë³´ì…ë‹ˆë‹¤. í•™ìŠµì— ì‚¬ìš©ëœ ìë£Œë‚˜ ë°©ë²•ë¡ ì— ëŒ€í•œ ê¹Šì€ ì´í•´ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. RAG ì±—ë´‡ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ê¸°ì´ˆ ê°œë…ì„ ë‹¤ì‹œ í™•ì¸í•´ ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤."
                else:
                    comment = "ì„±ì·¨ë„ëŠ” í˜„ì¬ ìˆ˜ì¤€ì„ ìœ ì§€í•  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤. ì •ì²´ê¸°ê°€ ë  ìˆ˜ ìˆìœ¼ë‹ˆ, **ìƒˆë¡œìš´ í•™ìŠµ ì½˜í…ì¸  í˜•ì‹(ì˜ˆ: ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´)ì„ ìƒì„±**í•˜ì—¬ í•™ìŠµì— í™œë ¥ì„ ë”í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ ë³´ì„¸ìš”."
            else: # English
                if avg_predict > avg_recent:
                    comment = "Based on recent learning data and LSTM prediction, **your achievement is projected to improve positively**. Maintain your current study methods or consider increasing the difficulty level."
                elif avg_predict < avg_recent - 5:
                    comment = "LSTM prediction suggests a **potential drop in achievement**. Your understanding of fundamental concepts may be lacking. Use the RAG Chatbot to review foundational knowledge."
                else:
                    comment = "Achievement is expected to remain stable. Consider generating **new content types (e.g., Practical Example Ideas)** to revitalize your learning during this plateau."


            st.info(comment)

        except Exception as e:
            st.error(f"LSTM Model Processing Error: {e}")
            st.info(L["lstm_disabled_error"])
