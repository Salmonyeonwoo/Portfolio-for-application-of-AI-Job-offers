# ========================================
# Streamlit AI í•™ìŠµ ì½”ì¹˜ (NLTK ì˜ì¡´ì„± ìš°íšŒ)
# ========================================
import streamlit as st
import os
import tempfile
import time
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredHTMLLoader # UnstructuredHTMLLoader ë‹¤ì‹œ ì¶”ê°€
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# [â­í•µì‹¬ ìˆ˜ì •â­] NLTK ì˜ì¡´ì„± ìš°íšŒë¥¼ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
# unstructuredê°€ NLTK ëŒ€ì‹  ë‹¤ë¥¸ íŒŒì„œë¥¼ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.
# NLTKë¥¼ ì™„ì „íˆ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
os.environ["UNSTRUCTURED_FORCE_ANON_USER_AGENT"] = "true" # ì‚¬ìš©ì ì—ì´ì „íŠ¸ ìµëª…í™”
os.environ["NLTK_DATA"] = "/tmp/nltk_data" # NLTK ë°ì´í„° ê²½ë¡œë¥¼ ì„ì‹œ ê²½ë¡œë¡œ ì„¤ì •

# NLTKë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„í¬íŠ¸í•˜ì§€ ì•Šë„ë¡ ì½”ë“œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

# ================================
# 1. LLM ë° ì„ë² ë”© ì´ˆê¸°í™” + ì„ë² ë”© ìºì‹œ
# (ì´ì „ ì½”ë“œì™€ ë™ì¼, LLM ì´ˆê¸°í™” ë¡œì§ ìœ ì§€)
# ================================
API_KEY = os.environ.get("GEMINI_API_KEY")

if 'client' not in st.session_state:
    if not API_KEY: # API_KEYê°€ ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ Noneì¸ ê²½ìš°
        st.error("âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.session_state.is_llm_ready = False
    else:
        try:
            st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, google_api_key=API_KEY)
            st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=API_KEY)
            st.session_state.is_llm_ready = True
        except Exception as e:
            st.error(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”. {e}")
            st.session_state.is_llm_ready = False

# LangChain ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)


# --- RAG ê´€ë ¨ í•¨ìˆ˜ ---
def get_document_chunks(files):
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•©ë‹ˆë‹¤."""
    documents = []
    temp_dir = tempfile.mkdtemp()

    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_filepath, "wb") as f:
            f.write(uploaded_file.getvalue())

        # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ ë¡œë” ì„ íƒ
        if uploaded_file.name.endswith(".pdf"):
            # PDFëŠ” PyPDFLoader ë˜ëŠ” UnstructuredPDFLoaderë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ, 
            # PyPDFLoaderëŠ” NLTKë¥¼ ì“°ì§€ ì•ŠìŒ. HTMLì€ UnstructuredHTML Loaderë¡œ ë³µêµ¬í•©ë‹ˆë‹¤.
            loader = PyPDFLoader(temp_filepath)
        elif uploaded_file.name.endswith(".html"):
            # HTML ì²˜ë¦¬ë¥¼ ìœ„í•´ UnstructuredHTML Loaderë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            loader = UnstructuredHTMLLoader(temp_filepath) 
        else:
            loader = TextLoader(temp_filepath, encoding="utf-8")

        documents.extend(loader.load())

    # í…ìŠ¤íŠ¸ ë¶„í•  (ì²­í‚¹)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    return text_splitter.split_documents(documents)

def get_vector_store(text_chunks):
    key = tuple(doc.page_content for doc in text_chunks)
    if key in st.session_state.embedding_cache:
        return st.session_state.embedding_cache[key]

    try:
        vector_store = FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)
        st.session_state.embedding_cache[key] = vector_store
        return vector_store
    except Exception as e:
        st.warning(f"ì„ë² ë”© ìš”ì²­ ì‹¤íŒ¨ (ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ê°€ëŠ¥): {e}")
        return None

def get_rag_chain(vector_store):
    if vector_store is None:
        return None
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )

# ================================
# 4. Streamlit UI
# ================================
st.set_page_config(page_title="ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜", layout="wide")

with st.sidebar:
    st.title("ğŸ“š AI Study Coach ì„¤ì •")
    st.markdown("---")

    uploaded_files = st.file_uploader(
        "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        type=["pdf","txt","html"],
        accept_multiple_files=True
    )

    if uploaded_files and st.session_state.is_llm_ready:
        if st.button("ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)", key="start_analysis"):
            with st.spinner("ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘..."):
                text_chunks = get_document_chunks(uploaded_files)
                vector_store = get_vector_store(text_chunks)
                if vector_store:
                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                    st.success(f"ì´ {len(text_chunks)}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!")
                else:
                    st.session_state.is_rag_ready = False
                    st.error("ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ.")

    else:
        st.session_state.is_rag_ready = False
        st.warning("ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    st.markdown("---")
    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        ["RAG ì§€ì‹ ì±—ë´‡", "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±", "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ"]
    )

st.title("âœ¨ ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜")

# ================================
# 5. ê¸°ëŠ¥ë³„ í˜ì´ì§€ êµ¬í˜„
# ================================
if feature_selection == "RAG ì§€ì‹ ì±—ë´‡":
    st.header("RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)")
    st.markdown("ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")
    if st.session_state.is_rag_ready and st.session_state.conversation_chain:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”"):
            st.session_state.messages.append({"role":"user","content":prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.conversation_chain.invoke({"question":prompt})
                        answer = response.get('answer','ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role":"assistant","content":answer})
                    except Exception as e:
                        st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}")
                        st.session_state.messages.append({"role":"assistant","content":"ì˜¤ë¥˜ ë°œìƒ"})
    else:
        st.warning("RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”.")

# ================================
# ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±
# ================================
elif feature_selection == "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±":
    st.header("ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±")
    st.markdown("í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±")

    if st.session_state.is_llm_ready:
        topic = st.text_input("í•™ìŠµ ì£¼ì œ")
        level = st.selectbox("ë‚œì´ë„", ["ì´ˆê¸‰","ì¤‘ê¸‰","ê³ ê¸‰"])
        content_type = st.selectbox("ì½˜í…ì¸  í˜•ì‹", ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸","ê°ê´€ì‹ í€´ì¦ˆ 3ë¬¸í•­","ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"])

        if st.button("ì½˜í…ì¸  ìƒì„±"):
            if topic:
                system_prompt = f"""ë‹¹ì‹ ì€ {level} ìˆ˜ì¤€ì˜ ì „ë¬¸ AI ì½”ì¹˜ì…ë‹ˆë‹¤.
ìš”ì²­ë°›ì€ ì£¼ì œì— ëŒ€í•´ {content_type} í˜•ì‹ì— ë§ì¶° ëª…í™•í•˜ê³  êµìœ¡ì ì¸ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œë§Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤."""

                user_prompt = f"ì£¼ì œ: {topic}. ìš”ì²­ í˜•ì‹: {content_type}."

                with st.spinner(f"{topic}ì— ëŒ€í•œ {content_type} ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.llm.invoke(
                            user_prompt,
                            system_instruction=system_prompt
                        )
                        st.success(f"**{topic}** - **{content_type}** ê²°ê³¼:")
                        st.markdown(response.content)
                    except Exception as e:
                        st.error(f"ì½˜í…ì¸  ìƒì„± ì˜¤ë¥˜: {e}")
            else:
                st.warning("í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")


