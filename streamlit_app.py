# ================================
# Streamlit App: ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜
# Features: RAG, LSTM ì˜ˆì¸¡, ë§ì¶¤ ì½˜í…ì¸  ìƒì„±
# Auto Local / Cloud Installation
# ================================

import os
import subprocess
import streamlit as st
import tempfile
import time
import nltk
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, UnstructuredHTMLLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import tensorflow as tf

# =====================================
# ğŸŒ í™˜ê²½ì— ë”°ë¼ TensorFlow & unstructured-inference ìë™ ì„¤ì¹˜
# =====================================
if not os.environ.get("STREAMLIT_RUNTIME"):
    try:
        subprocess.check_call([
            "pip", "install",
            "tensorflow==2.13.0",
            "unstructured-inference==0.7.11"
        ])
        print("âœ… Local mode detected: Installed TensorFlow & unstructured-inference")
    except Exception as e:
        print("âš ï¸ Local install skipped:", e)
else:
    print("ğŸŒ Streamlit Cloud mode detected: Skipping heavy installs")

# =====================================
# NLTK punkt ë‹¤ìš´ë¡œë“œ (RAGì—ì„œ í•„ìš”)
# =====================================
nltk.download('punkt', quiet=True)

# =====================================
# Gemini API í‚¤
# =====================================
API_KEY = os.environ.get("GEMINI_API_KEY", "YOUR_GEMINI_API_KEY")

# =====================================
# LangChain LLM & Embedding ì´ˆê¸°í™”
# =====================================
if 'client' not in st.session_state:
    try:
        st.session_state.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.7,
            google_api_key=API_KEY
        )
        st.session_state.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=API_KEY
        )
        st.session_state.is_llm_ready = True
    except Exception as e:
        st.error(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        st.session_state.is_llm_ready = False

# =====================================
# LangChain Memory ì´ˆê¸°í™”
# =====================================
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )

# =====================================
# LSTM ëª¨ë¸ ìƒì„± í•¨ìˆ˜
# =====================================
@st.cache_resource
def load_or_train_lstm():
    np.random.seed(42)
    data = np.cumsum(np.random.normal(loc=5, scale=5, size=50)) + 60
    data = np.clip(data, 50, 95)

    def create_dataset(dataset, look_back=3):
        X, Y = [], []
        for i in range(len(dataset) - look_back):
            X.append(dataset[i:(i + look_back)])
            Y.append(dataset[i + look_back])
        return np.array(X), np.array(Y)

    look_back = 5
    X, Y = create_dataset(data, look_back)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    model = Sequential([
        LSTM(50, activation='relu', input_shape=(look_back, 1)),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X, Y, epochs=10, batch_size=1, verbose=0)
    return model, data

# =====================================
# RAG ê´€ë ¨ í•¨ìˆ˜
# =====================================
def get_document_chunks(files):
    documents = []
    temp_dir = tempfile.mkdtemp()

    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_filepath, "wb") as f:
            f.write(uploaded_file.getvalue())

        if uploaded_file.name.endswith(".pdf"):
            loader = PyPDFLoader(temp_filepath)
        elif uploaded_file.name.endswith(".html"):
            loader = UnstructuredHTMLLoader(temp_filepath)
        else:
            loader = TextLoader(temp_filepath, encoding="utf-8")

        documents.extend(loader.load())

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    return text_splitter.split_documents(documents)


def get_vector_store(text_chunks):
    return FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)


def get_rag_chain(vector_store):
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )

# =====================================
# Streamlit UI
# =====================================
st.set_page_config(page_title="ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜", layout="wide")

with st.sidebar:
    st.title("ğŸ“š AI Study Coach ì„¤ì •")
    st.markdown("---")

    uploaded_files = st.file_uploader(
        "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        type=["pdf", "txt", "html"],
        accept_multiple_files=True
    )

    if uploaded_files and st.session_state.is_llm_ready:
        if st.button("ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)", key="start_analysis"):
            with st.spinner("ìë£Œë¥¼ ë¶„ì„í•˜ê³  í•™ìŠµ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶• ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    text_chunks = get_document_chunks(uploaded_files)
                    vector_store = get_vector_store(text_chunks)
                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                    st.success(f"ì´ {len(text_chunks)}ê°œ ì²­í¬ë¡œ í•™ìŠµ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"RAG êµ¬ì¶• ì˜¤ë¥˜: {e}")
                    st.session_state.is_rag_ready = False
    else:
        st.session_state.is_rag_ready = False
        st.warning("ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")

    st.markdown("---")
    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        ["RAG ì§€ì‹ ì±—ë´‡", "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±", "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ"]
    )

st.title("âœ¨ ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜")

# =====================================
# ê¸°ëŠ¥ë³„ êµ¬í˜„
# =====================================
if feature_selection == "RAG ì§€ì‹ ì±—ë´‡":
    st.header("RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)")
    st.markdown("ì—…ë¡œë“œëœ ë¬¸ì„œ(í¬íŠ¸í´ë¦¬ì˜¤, PDF ë“±)ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")

    if st.session_state.is_rag_ready:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš” (ì˜ˆ: ì´ ë¬¸ì„œì˜ í•µì‹¬ ê¸°ìˆ ì€ ë¬´ì—‡ì¸ê°€ìš”?)"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    try:
                        response = st.session_state.conversation_chain.invoke({"question": prompt})
                        answer = response.get('answer', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role": "assistant", "content": answer})
                    except Exception as e:
                        st.error(f"ì±—ë´‡ ì‘ë‹µ ì˜¤ë¥˜: {e}")
                        st.session_state.messages.append({"role": "assistant", "content": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."})
    else:
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  'ìë£Œ ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

elif feature_selection == "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±":
    st.header("ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±")
    st.markdown("ì›í•˜ëŠ” í•™ìŠµ ì£¼ì œ, ë‚œì´ë„, í˜•ì‹ì„ ì…ë ¥í•˜ì‹œë©´ LLMì´ ë§ì¶¤í˜• ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    if st.session_state.is_llm_ready:
        topic = st.text_input("í•™ìŠµ ì£¼ì œ (ì˜ˆ: Transformerì˜ Self-Attention ë©”ì»¤ë‹ˆì¦˜)")
        level = st.selectbox("ë‚œì´ë„", ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"])
        content_type = st.selectbox("ì½˜í…ì¸  í˜•ì‹", ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 3ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"])

        if st.button("ì½˜í…ì¸  ìƒì„±"):
            if topic:
                system_prompt = f"ë‹¹ì‹ ì€ {level} ìˆ˜ì¤€ì˜ ì „ë¬¸ AI ì½”ì¹˜ì…ë‹ˆë‹¤. ìš”ì²­ ì£¼ì œì— ëŒ€í•´ {content_type} í˜•ì‹ìœ¼ë¡œ êµìœ¡ì ì¸ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€."
                user_prompt = f"ì£¼ì œ: {topic}. ìš”ì²­ í˜•ì‹: {content_type}."
                with st.spinner(f"{topic}ì— ëŒ€í•œ {content_type} ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.llm.invoke(user_prompt, system_instruction=system_prompt)
                        st.success(f"**{topic}** ì— ëŒ€í•œ **{content_type}** ê²°ê³¼:")
                        st.markdown(response.content)
                    except Exception as e:
                        st.error(f"ì½˜í…ì¸  ìƒì„± ì˜¤ë¥˜: {e}")
            else:
                st.warning("í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        st.error("LLM ì´ˆê¸°í™” ì‹¤íŒ¨. API í‚¤ í™•ì¸ í•„ìš”.")

elif feature_selection == "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ":
    st.header("LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ")
    st.markdown("ê°€ìƒì˜ ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¯¸ë˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")

    with st.spinner("LSTM ëª¨ë¸ ë¡œë“œ/í•™ìŠµ ì¤‘..."):
        try:
            lstm_model, historical_scores = load_or_train_lstm()
            st.success("LSTM ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

            look_back = 5
            last_sequence = historical_scores[-look_back:]
            input_sequence = np.reshape(last_sequence, (1, look_back, 1))

            future
