import streamlit as st
import os
import tempfile
import time
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, UnstructuredHTMLLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# --- 4. Streamlit UI êµ¬ì„± (ìµœìƒë‹¨ìœ¼ë¡œ ì´ë™) ---
st.set_page_config(page_title="ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜", layout="wide")

# --- TensorFlow/LSTM ê´€ë ¨ ì½”ë“œ ì„ì‹œ ì œê±° ---
# Streamlit Cloud ë°°í¬ ì„±ê³µì„ ìœ„í•´ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë¡œì§ì„ ëª¨ë‘ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.

# --- 1. í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ì´ˆê¸°í™” ---

# Gemini API í‚¤ ì„¤ì • (secrets.tomlì—ì„œ ë¡œë“œ)
API_KEY = os.environ.get("GEMINI_API_KEY")

if 'client' not in st.session_state:
    if not API_KEY: # API_KEYê°€ ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ Noneì¸ ê²½ìš°
        st.error("âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.session_state.is_llm_ready = False
    else:
        try:
            # LLM ë° Embedding ëª¨ë¸ ì´ˆê¸°í™”
            st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, google_api_key=API_KEY)
            st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=API_KEY)
            st.session_state.is_llm_ready = True
        except Exception as e:
            st.error(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”. {e}")
            st.session_state.is_llm_ready = False

# LangChain ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)


# --- 2. LSTM ëª¨ë¸ ì •ì˜ (ê¸°ëŠ¥ ì„ì‹œ ì£¼ì„ ì²˜ë¦¬) ---
# LSTM ê´€ë ¨ í•¨ìˆ˜ëŠ” ëª¨ë‘ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ì—ëŸ¬ ë°©ì§€
# def load_or_train_lstm():
#     return None, None 


# --- 3. RAG ê´€ë ¨ í•¨ìˆ˜ ---
def get_document_chunks(files):
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•©ë‹ˆë‹¤."""
    documents = []
    temp_dir = tempfile.mkdtemp()

    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_filepath, "wb") as f:
            f.write(uploaded_file.getvalue())

        # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ ë¡œë” ì„ íƒ
        if uploaded_file.name.endswith(".pdf"):
            # PDF ë¡œë”© ì‹œ unstructuredê°€ NLTK ëŒ€ì‹  PaddlePaddleì„ ì‚¬ìš©í•˜ë„ë¡ ê¸°ëŒ€í•©ë‹ˆë‹¤.
            loader = PyPDFLoader(temp_filepath)
        elif uploaded_file.name.endswith(".html"):
            loader = UnstructuredHTMLLoader(temp_filepath)
        else:
            loader = TextLoader(temp_filepath, encoding="utf-8")

        documents.extend(loader.load())

    # í…ìŠ¤íŠ¸ ë¶„í•  (ì²­í‚¹)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    return text_splitter.split_documents(documents)


def get_vector_store(text_chunks):
    """í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  Vector Storeë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    # FAISSê°€ requirements.txtì— í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    try:
        vector_store = FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)
        return vector_store
    except ImportError:
        # FAISS ì„¤ì¹˜ ì˜¤ë¥˜ ì‹œ ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´
        st.error("FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. requirements.txtì— 'faiss-cpu'ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        return None
    except Exception as e:
        st.error(f"Vector Store ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def get_rag_chain(vector_store):
    """ê²€ìƒ‰ ì²´ì¸(ConversationalRetrievalChain)ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if vector_store is None:
        return None
        
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )


# --- 4. Streamlit UI (ì‚¬ì´ë“œë°” ë° ê¸°ëŠ¥ ì„ íƒ) ---

# ì‚¬ì´ë“œë°”: ì„¤ì • ë° íŒŒì¼ ì—…ë¡œë“œ
with st.sidebar:
    st.title("ğŸ“š AI Study Coach ì„¤ì •")
    st.markdown("---")

    uploaded_files = st.file_uploader(
        "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        type=["pdf", "txt", "html"],
        accept_multiple_files=True
    )

    # LLM ë° RAG ìƒíƒœ ê´€ë¦¬
    if uploaded_files and st.session_state.is_llm_ready: # is_nltk_ready ì²´í¬ ì œê±°
        if st.button("ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)", key="start_analysis"):
            with st.spinner("ìë£Œë¥¼ ë¶„ì„í•˜ê³  í•™ìŠµ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶• ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    text_chunks = get_document_chunks(uploaded_files)
                    vector_store = get_vector_store(text_chunks)
                    
                    if vector_store:
                        st.session_state.conversation_chain = get_rag_chain(vector_store)
                        st.session_state.is_rag_ready = True
                        st.success(f"ì´ {len(text_chunks)}ê°œ ì²­í¬ë¡œ í•™ìŠµ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
                    else:
                         st.session_state.is_rag_ready = False
                         
                except Exception as e:
                    st.error(f"RAG êµ¬ì¶• ì˜¤ë¥˜: {e}")
                    st.session_state.is_rag_ready = False
    else:
        st.session_state.is_rag_ready = False
        st.warning("ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")

    # ê¸°ëŠ¥ ì„ íƒ ë¼ë””ì˜¤ ë²„íŠ¼
    st.markdown("---")
    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        ["ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±", "RAG ì§€ì‹ ì±—ë´‡", "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ"]
    )

# ë©”ì¸ í™”ë©´ ì œëª©
st.title("âœ¨ ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜")

# --- 5. ê¸°ëŠ¥ë³„ í˜ì´ì§€ êµ¬í˜„ ---
# (ì´í•˜ ìƒëµ - ì´ì „ ì½”ë“œì™€ ë™ì¼)

if feature_selection == "RAG ì§€ì‹ ì±—ë´‡":
    # RAG ì±—ë´‡ ê¸°ëŠ¥ 
    st.header("RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)")
    st.markdown("ì—…ë¡œë“œëœ ë¬¸ì„œ(í¬íŠ¸í´ë¦¬ì˜¤, PDF ë“±)ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")

    if st.session_state.is_rag_ready:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš” (ì˜ˆ: ì´ ë¬¸ì„œì˜ í•µì‹¬ ê¸°ìˆ ì€ ë¬´ì—‡ì¸ê°€ìš”?"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    try:
                        # RAG ì²´ì¸ì€ ConversationalRetrievalChainì„ ì‚¬ìš©í•˜ë¯€ë¡œ system_instruction ë¬¸ì œ ì—†ìŒ
                        response = st.session_state.conversation_chain.invoke({"question": prompt})
                        answer = response.get('answer', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role": "assistant", "content": answer})
                    except Exception as e:
                        st.error(f"ì±—ë´‡ ì‘ë‹µ ì˜¤ë¥˜: {e}")
                        st.session_state.messages.append({"role": "assistant", "content": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."})
    else:
        st.error("RAG ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´, ì‚¬ì´ë“œë°”ì—ì„œ í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  'ìë£Œ ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")


elif feature_selection == "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±":
    # ì½˜í…ì¸  ìƒì„± ê¸°ëŠ¥
    st.header("ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±")
    st.markdown("ì›í•˜ëŠ” í•™ìŠµ ì£¼ì œ, ë‚œì´ë„, í˜•ì‹ì„ ì…ë ¥í•˜ì‹œë©´ LLMì´ ë§ì¶¤í˜• ì½˜í…ì¸ (ìš”ì•½, í€´ì¦ˆ)ë¥¼ ìƒì„±í•´ ë“œë¦½ë‹ˆë‹¤.")

    if st.session_state.is_llm_ready:
        topic = st.text_input("í•™ìŠµ ì£¼ì œ (ì˜ˆ: Transformerì˜ Self-Attention ë©”ì»¤ë‹ˆì¦˜)")
        level = st.selectbox("ë‚œì´ë„", ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"])
        content_type = st.selectbox("ì½˜í…ì¸  í˜•ì‹", ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 3ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"])

        if st.button("ì½˜í…ì¸  ìƒì„±"):
            if topic:
                # ğŸ› ï¸ ìˆ˜ì •ëœ ë¶€ë¶„: system_instructionì„ user_promptì— í†µí•©í•©ë‹ˆë‹¤.
                system_prompt = f"""ë‹¹ì‹ ì€ {level} ìˆ˜ì¤€ì˜ ì „ë¬¸ AI ì½”ì¹˜ì…ë‹ˆë‹¤. ìš”ì²­ë°›ì€ ì£¼ì œì— ëŒ€í•´ {content_type} í˜•ì‹ì— ë§ì¶° ëª…í™•í•˜ê³  êµìœ¡ì ì¸ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”. ë‹µë³€ì€ í•œêµ­ì–´ë¡œë§Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤."""
                
                # í”„ë¡¬í”„íŠ¸ í†µí•© (System + User)
                full_prompt = f"{system_prompt}\n\n[ì‚¬ìš©ì ìš”ì²­]\nì£¼ì œ: {topic}. ìš”ì²­ í˜•ì‹: {content_type}."

                with st.spinner(f"{topic}ì— ëŒ€í•œ {content_type}ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    try:
                        # LLMì— ìš”ì²­: system_instruction ì¸ìˆ˜ë¥¼ ì œê±°í•˜ê³  í†µí•©ëœ í”„ë¡¬í”„íŠ¸ë§Œ ì „ë‹¬
                        response = st.session_state.llm.invoke(full_prompt)
                        st.success(f"**{topic}** ì— ëŒ€í•œ **{content_type}** ê²°ê³¼:")
                        st.markdown(response.content)

                    except Exception as e:
                        st.error(f"ì½˜í…ì¸  ìƒì„± ì˜¤ë¥˜: {e}")
            else:
                st.warning("í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        st.error("LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")

elif feature_selection == "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ":
    # LSTM ê¸°ëŠ¥ ë¹„í™œì„±í™” ë©”ì‹œì§€ ì¶œë ¥ (TensorFlow ì˜¤ë¥˜ ë°©ì§€)
    st.header("LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ")
    st.markdown("LSTM ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ TensorFlow ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì„¤ì¹˜ê°€ ì„ í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
    st.error("í˜„ì¬ ë¹Œë“œ í™˜ê²½ ë¬¸ì œë¡œ ì¸í•´ LSTM ê¸°ëŠ¥ì€ ì ì •ì ìœ¼ë¡œ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. 'ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±' ê¸°ëŠ¥ì„ ë¨¼ì € ì‚¬ìš©í•´ ì£¼ì„¸ìš”.")
