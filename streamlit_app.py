# =======================================
# Streamlit + LangChain + Gemini ì•ˆì •í™”
# ë¬´ë£Œ í‹°ì–´ ì„ë² ë”© ìºì‹œ í¬í•¨
# =======================================

import os
import pickle
import tempfile
import subprocess
import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, UnstructuredHTMLLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory

import nltk

# =======================================
# 0. NLTK ë¦¬ì†ŒìŠ¤ ìë™ ë‹¤ìš´ë¡œë“œ
# =======================================
if "nltk_downloaded" not in st.session_state:
    nltk.download('punkt')
    nltk.download('averaged_perceptron_tagger_eng')
    st.session_state["nltk_downloaded"] = True

# =======================================
# 1. LLM ë° Embeddings ì´ˆê¸°í™”
# =======================================
API_KEY = os.environ.get("GEMINI_API_KEY", "YOUR_GEMINI_API_KEY")

if "llm" not in st.session_state:
    try:
        st.session_state.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.7,
            google_api_key=API_KEY
        )
        st.session_state.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=API_KEY
        )
        st.session_state.is_llm_ready = True
    except Exception as e:
        st.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.session_state.is_llm_ready = False

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# =======================================
# 2. LSTM ëª¨ë¸ ë¡œë“œ/í•™ìŠµ
# =======================================
@st.cache_resource
def load_or_train_lstm():
    np.random.seed(42)
    data = np.cumsum(np.random.normal(5, 5, 50)) + 60
    data = np.clip(data, 50, 95)

    def create_dataset(dataset, look_back=3):
        X, Y = [], []
        for i in range(len(dataset)-look_back):
            X.append(dataset[i:i+look_back])
            Y.append(dataset[i+look_back])
        return np.array(X), np.array(Y)

    look_back = 5
    X, Y = create_dataset(data, look_back)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    model = Sequential([LSTM(50, activation="relu", input_shape=(look_back, 1)), Dense(1)])
    model.compile(optimizer="adam", loss="mse")
    model.fit(X, Y, epochs=10, batch_size=1, verbose=0)
    return model, data

# =======================================
# 3. RAG ë¬¸ì„œ ì²˜ë¦¬ + VectorStore ìºì‹œ
# =======================================
CACHE_PATH = "vectorstore_cache.pkl"

def get_document_chunks(files):
    documents = []
    temp_dir = tempfile.mkdtemp()
    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_filepath, "wb") as f:
            f.write(uploaded_file.getvalue())

        if uploaded_file.name.endswith(".pdf"):
            loader = PyPDFLoader(temp_filepath)
        elif uploaded_file.name.endswith(".html"):
            loader = UnstructuredHTMLLoader(temp_filepath)
        else:
            loader = TextLoader(temp_filepath, encoding="utf-8")

        documents.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    return splitter.split_documents(documents)

def get_vector_store(text_chunks):
    try:
        vector_store = FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)
        # ìºì‹œ ì €ì¥
        with open(CACHE_PATH, "wb") as f:
            pickle.dump(vector_store, f)
        return vector_store
    except Exception as e:
        st.warning(f"ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ.\n{e}")
        if os.path.exists(CACHE_PATH):
            with open(CACHE_PATH, "rb") as f:
                st.info("âš¡ ìºì‹œëœ VectorStore ì‚¬ìš©")
                return pickle.load(f)
        else:
            st.error("RAG êµ¬ì¶• ì‹¤íŒ¨: í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            return None

def get_rag_chain(vector_store):
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )

# =======================================
# 4. Streamlit UI
# =======================================
st.set_page_config(page_title="AI í•™ìŠµ ì½”ì¹˜", layout="wide")

with st.sidebar:
    st.title("ğŸ“š ì„¤ì •")
    uploaded_files = st.file_uploader(
        "ìë£Œ ì—…ë¡œë“œ (PDF/TXT/HTML)", type=["pdf","txt","html"], accept_multiple_files=True
    )

    if uploaded_files and st.session_state.is_llm_ready:
        if st.button("ìë£Œ ë¶„ì„ (RAG êµ¬ì¶•)"):
            with st.spinner("ìë£Œ ë¶„ì„ ì¤‘..."):
                chunks = get_document_chunks(uploaded_files)
                vector_store = get_vector_store(chunks)
                if vector_store:
                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                    st.success(f"RAG êµ¬ì¶• ì™„ë£Œ! ì´ {len(chunks)} ì²­í¬.")
                else:
                    st.session_state.is_rag_ready = False
    else:
        st.session_state.is_rag_ready = False

    st.markdown("---")
    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        ["RAG ì±—ë´‡", "ë§ì¶¤í˜• ì½˜í…ì¸  ìƒì„±", "LSTM ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ"]
    )

st.title("âœ¨ ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜")

# =======================================
# 5. ê¸°ëŠ¥ë³„ êµ¬í˜„
# =======================================
if feature_selection == "RAG ì±—ë´‡":
    st.header("RAG ê¸°ë°˜ ë¬¸ì„œ Q&A")
    if st.session_state.is_rag_ready:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
            st.session_state.messages.append({"role":"user","content":prompt})
            with st.chat_message("user"): st.markdown(prompt)
            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        resp = st.session_state.conversation_chain.invoke({"question":prompt})
                        answer = resp.get("answer","ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
                        st.markdown(answer)
                        st.session_state.messages.append({"role":"assistant","content":answer})
                    except Exception as e:
                        st.error(f"RAG ì˜¤ë¥˜: {e}")

elif feature_selection == "ë§ì¶¤í˜• ì½˜í…ì¸  ìƒì„±":
    st.header("ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±")
    if st.session_state.is_llm_ready:
        topic = st.text_input("í•™ìŠµ ì£¼ì œ")
        level = st.selectbox("ë‚œì´ë„", ["ì´ˆê¸‰","ì¤‘ê¸‰","ê³ ê¸‰"])
        content_type = st.selectbox("í˜•ì‹", ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸","ê°ê´€ì‹ í€´ì¦ˆ 3ë¬¸í•­","ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"])
        if st.button("ì½˜í…ì¸  ìƒì„±"):
            if topic:
                system_prompt = f"""ë‹¹ì‹ ì€ {level} ìˆ˜ì¤€ì˜ ì „ë¬¸ AI ì½”ì¹˜ì…ë‹ˆë‹¤.
ìš”ì²­ ì£¼ì œì— ëŒ€í•´ {content_type} í˜•ì‹ì— ë§ì¶° ëª…í™•í•˜ê³  êµìœ¡ì ì¸ ì½˜í…ì¸ ë¥¼ ì œê³µí•˜ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•©ë‹ˆë‹¤."""
                user_prompt = f"ì£¼ì œ: {topic}. í˜•ì‹: {content_type}."
                with st.spinner("ìƒì„± ì¤‘..."):
                    try:
                        resp = st.session_state.llm.invoke(user_prompt, system_instruction=system_prompt)
                        st.success(f"**{topic}** - {content_type} ê²°ê³¼:")
                        st.markdown(resp.content)
                    except Exception as e:
                        st.error(f"ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨: {e}")
            else:
                st.warning("í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

elif feature_selection == "LSTM ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ":
    st.header("LSTM í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡")
    with st.spinner("LSTM ëª¨ë¸ ë¡œë“œ ì¤‘..."):
        try:
            lstm_model, historical_scores = load_or_train_lstm()
            st.success("LSTM ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

            look_back = 5
            last_seq = historical_scores[-look_back:]
            input_seq = np.reshape(last_seq, (1, look_back, 1))

            future_preds = []
            curr_input = input_seq
            for _ in range(5):
                next_score = lstm_model.predict(curr_input, verbose=0)[0]
                future_preds.append(next_score[0])
                curr_input = np.append(curr_input[:,1:,:], next_score[0]).reshape(1, look_back, 1)

            fig, ax = plt.subplots(figsize=(10,6))
            ax.plot(range(len(historical_scores)), historical_scores, label="ê³¼ê±° ì ìˆ˜", marker='o', linestyle='-')
            future_idx = range(len(historical_scores), len(historical_scores)+len(future_preds))
            ax.plot(future_idx, future_preds, label="ì˜ˆì¸¡ ì ìˆ˜", marker='x', linestyle='--', color='red')
            ax.set_title("LSTM ì„±ì·¨ë„ ì˜ˆì¸¡")
            ax.set_xlabel("ì£¼ê¸°")
            ax.set_ylabel("ì ìˆ˜")
            ax.legend()
            ax.grid(True)
            st.pyplot(fig)

            avg_recent = np.mean(historical_scores[-5:])
            avg_future = np.mean(future_preds)
            if avg_future > avg_recent:
                comment = "ì•ìœ¼ë¡œ ì„±ì·¨ë„ í–¥ìƒ ì˜ˆìƒ"
            elif avg_future < avg_recent-5:
                comment = "ì„±ì·¨ë„ í•˜ë½ ê°€ëŠ¥. RAG ì±—ë´‡ í™œìš© ì¶”ì²œ"
            else:
                comment = "ì„±ì·¨ë„ ìœ ì§€ ì˜ˆìƒ"
            st.info(comment)

        except Exception as e:
            st.error(f"LSTM ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
