# ================================
# Streamlit AI í•™ìŠµ ì½”ì¹˜ ìµœì¢… ë²„ì „
# ================================

import streamlit as st

# âš ï¸ ê°€ì¥ ë¨¼ì € í˜ì´ì§€ ì„¤ì • (StreamlitAPIException ë°©ì§€)
st.set_page_config(
    page_title="ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜",
    layout="wide"
)

import os
import subprocess
import tempfile
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# LangChain & Google Gemini
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, UnstructuredHTMLLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory

# TensorFlow
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.applications import MobileNetV3Large

# NLTK punkt ë¦¬ì†ŒìŠ¤ ìë™ ì„¤ì¹˜
import nltk
try:
    nltk.data.find("tokenizers/punkt")
except:
    nltk.download("punkt")

# ğŸŒ ë¡œì»¬ í™˜ê²½ ì „ìš© ì„¤ì¹˜
if not os.environ.get("STREAMLIT_RUNTIME"):
    try:
        subprocess.check_call([
            "pip", "install",
            "tensorflow==2.13.0",
            "unstructured-inference==0.7.11"
        ])
        print("âœ… Local mode: TensorFlow & unstructured-inference ì„¤ì¹˜ ì™„ë£Œ")
    except Exception as e:
        print("âš ï¸ Local install skipped:", e)
else:
    print("ğŸŒ Streamlit Cloud mode: Heavy install skipped")

# ================================
# LLM & Memory ì´ˆê¸°í™”
# ================================

API_KEY = os.environ.get("GEMINI_API_KEY", "YOUR_GEMINI_API_KEY")

if 'client' not in st.session_state:
    try:
        st.session_state.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.7,
            google_api_key=API_KEY
        )
        st.session_state.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=API_KEY
        )
        st.session_state.is_llm_ready = True
    except Exception as e:
        st.error(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        st.session_state.is_llm_ready = False

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ================================
# LSTM ëª¨ë¸ ì •ì˜ (ì‹¬í™” ê¸°ëŠ¥)
# ================================

@st.cache_resource
def load_or_train_lstm():
    np.random.seed(42)
    data = np.cumsum(np.random.normal(loc=5, scale=5, size=50)) + 60
    data = np.clip(data, 50, 95)

    def create_dataset(dataset, look_back=3):
        X, Y = [], []
        for i in range(len(dataset) - look_back):
            X.append(dataset[i:(i + look_back)])
            Y.append(dataset[i + look_back])
        return np.array(X), np.array(Y)

    look_back = 5
    X, Y = create_dataset(data, look_back)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    model = Sequential([
        LSTM(50, activation='relu', input_shape=(look_back, 1)),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X, Y, epochs=10, batch_size=1, verbose=0)

    return model, data

# ================================
# RAG êµ¬ì¶• ê´€ë ¨ í•¨ìˆ˜
# ================================

def get_document_chunks(files):
    documents = []
    temp_dir = tempfile.mkdtemp()

    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_filepath, "wb") as f:
            f.write(uploaded_file.getvalue())

        if uploaded_file.name.endswith(".pdf"):
            loader = PyPDFLoader(temp_filepath)
        elif uploaded_file.name.endswith(".html"):
            loader = UnstructuredHTMLLoader(temp_filepath)
        else:
            loader = TextLoader(temp_filepath, encoding="utf-8")

        documents.extend(loader.load())

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    return text_splitter.split_documents(documents)


def get_vector_store(text_chunks):
    return FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)


def get_rag_chain(vector_store):
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )

# ================================
# Streamlit UI
# ================================

with st.sidebar:
    st.title("ğŸ“š AI Study Coach ì„¤ì •")
    st.markdown("---")
    uploaded_files = st.file_uploader(
        "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        type=["pdf", "txt", "html"],
        accept_multiple_files=True
    )

    if uploaded_files and st.session_state.is_llm_ready:
        if st.button("ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)", key="start_analysis"):
            with st.spinner("ìë£Œ ë¶„ì„ ì¤‘..."):
                try:
                    text_chunks = get_document_chunks(uploaded_files)
                    vector_store = get_vector_store(text_chunks)
                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                    st.success(f"ì´ {len(text_chunks)}ê°œ ì²­í¬ ë¶„ì„ ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"RAG êµ¬ì¶• ì˜¤ë¥˜: {e}")
                    st.session_state.is_rag_ready = False
    else:
        st.session_state.is_rag_ready = False

    st.markdown("---")
    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        ["RAG ì§€ì‹ ì±—ë´‡", "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±", "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ"]
    )

st.title("âœ¨ ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜")

# ================================
# ê¸°ëŠ¥ë³„ í˜ì´ì§€
# ================================

if feature_selection == "RAG ì§€ì‹ ì±—ë´‡":
    st.header("RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)")
    st.markdown("ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")
    if st.session_state.is_rag_ready:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.conversation_chain.invoke({"question": prompt})
                        answer = response.get('answer', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role": "assistant", "content": answer})
                    except Exception as e:
                        st.error(f"ì±—ë´‡ ì‘ë‹µ ì˜¤ë¥˜: {e}")
                        st.session_state.messages.append({"role": "assistant", "content": "ì˜¤ë¥˜ ë°œìƒ"})

elif feature_selection == "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±":
    st.header("ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±")
    if st.session_state.is_llm_ready:
        topic = st.text_input("í•™ìŠµ ì£¼ì œ")
        level = st.selectbox("ë‚œì´ë„", ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"])
        content_type = st.selectbox("ì½˜í…ì¸  í˜•ì‹", ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 3ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"])

        if st.button("ì½˜í…ì¸  ìƒì„±"):
            if topic:
                system_prompt = f"ë‹¹ì‹ ì€ {level} ìˆ˜ì¤€ì˜ ì „ë¬¸ AI ì½”ì¹˜ì…ë‹ˆë‹¤. ìš”ì²­ ì£¼ì œ: {topic}. í˜•ì‹: {content_type}. í•œêµ­ì–´ë¡œ ì‘ì„±."
                user_prompt = f"ì£¼ì œ: {topic}. í˜•ì‹: {content_type}."
                with st.spinner("ì½˜í…ì¸  ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.llm.invoke(user_prompt, system_instruction=system_prompt)
                        st.markdown(response.content)
                    except Exception as e:
                        st.error(f"ì½˜í…ì¸  ìƒì„± ì˜¤ë¥˜: {e}")
            else:
                st.warning("í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        st.error("LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

elif feature_selection == "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ":
    st.header("LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡")
    with st.spinner("LSTM ëª¨ë¸ í•™ìŠµ ì¤‘..."):
        try:
            lstm_model, historical_scores = load_or_train_lstm()
            st.success("LSTM ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

            look_back = 5
            last_sequence = historical_scores[-look_back:]
            input_sequence = np.reshape(last_sequence, (1, look_back, 1))

            future_predictions = []
            current_input = input_sequence

            for _ in range(5):
                next_score = lstm_model.predict(current_input, verbose=0)[0]
                future_predictions.append(next_score[0])
                next_input = np.append(current_input[:, 1:, :], next_score[0]).reshape(1, look_back, 1)
                current_input = next_input

            fig, ax = plt.subplots(figsize=(10, 6))
            ax.plot(range(len(historical_scores)), historical_scores, label="ê³¼ê±° ì ìˆ˜", marker='o')
            future_indices = range(len(historical_scores), len(historical_scores) + len(future_predictions))
            ax.plot(future_indices, future_predictions, label="ì˜ˆì¸¡ ì„±ì·¨ë„ (ë‹¤ìŒ 5ì¼)", marker='x', linestyle='--', color='red')


