# ================================
# Streamlit Cloud + Local Dual Mode
# LangChain + Gemini (2025-11)
# ================================

import os
import subprocess
import streamlit as st
import tempfile
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, UnstructuredHTMLLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory

# --- 0. ë¡œì»¬ í™˜ê²½ ìë™ ì„¤ì¹˜ ---
if not os.environ.get("STREAMLIT_RUNTIME"):
    try:
        subprocess.check_call([
            "pip", "install",
            "tensorflow==2.13.0",
            "unstructured-inference==0.7.11"
        ])
        print("âœ… Local mode: Installed TensorFlow & unstructured-inference")
    except Exception as e:
        print("âš ï¸ Local install skipped:", e)
else:
    print("ğŸŒ Streamlit Cloud detected: Skipping heavy installs")

# ================================
# 1. LLM ì´ˆê¸°í™”
# ================================
API_KEY = os.environ.get("GEMINI_API_KEY", "YOUR_GEMINI_API_KEY")

if 'llm' not in st.session_state:
    try:
        st.session_state.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.7,
            google_api_key=API_KEY
        )
        st.session_state.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=API_KEY
        )
        st.session_state.is_llm_ready = True
    except Exception as e:
        st.error(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        st.session_state.is_llm_ready = False

# Memory
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ================================
# 2. LSTM ëª¨ë¸ ì •ì˜
# ================================
@st.cache_resource
def load_or_train_lstm():
    np.random.seed(42)
    data = np.cumsum(np.random.normal(5, 5, 50)) + 60
    data = np.clip(data, 50, 95)

    def create_dataset(dataset, look_back=3):
        X, Y = [], []
        for i in range(len(dataset)-look_back):
            X.append(dataset[i:i+look_back])
            Y.append(dataset[i+look_back])
        return np.array(X), np.array(Y)

    look_back = 5
    X, Y = create_dataset(data, look_back)
    X = X.reshape(X.shape[0], X.shape[1], 1)

    model = Sequential([
        LSTM(50, activation='relu', input_shape=(look_back,1)),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X, Y, epochs=10, batch_size=1, verbose=0)
    return model, data

# ================================
# 3. RAG í•¨ìˆ˜
# ================================
def get_document_chunks(files):
    documents = []
    temp_dir = tempfile.mkdtemp()
    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_filepath, "wb") as f:
            f.write(uploaded_file.getvalue())

        if uploaded_file.name.endswith(".pdf"):
            loader = PyPDFLoader(temp_filepath)
        elif uploaded_file.name.endswith(".html"):
            loader = UnstructuredHTMLLoader(temp_filepath)
        else:
            loader = TextLoader(temp_filepath, encoding="utf-8")

        documents.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    return splitter.split_documents(documents)

def get_vector_store(chunks):
    return FAISS.from_documents(chunks, embedding=st.session_state.embeddings)

def get_rag_chain(vector_store):
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )

# ================================
# 4. Streamlit UI
# ================================
st.set_page_config(page_title="AI í•™ìŠµ ì½”ì¹˜", layout="wide")

with st.sidebar:
    st.title("ğŸ“š AI Study Coach ì„¤ì •")
    st.markdown("---")

    uploaded_files = st.file_uploader(
        "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        type=["pdf", "txt", "html"],
        accept_multiple_files=True
    )

    if uploaded_files and st.session_state.is_llm_ready:
        if st.button("ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)"):
            with st.spinner("RAG ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘..."):
                try:
                    chunks = get_document_chunks(uploaded_files)
                    vector_store = get_vector_store(chunks)
                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                    st.success(f"{len(chunks)}ê°œ ì²­í¬ë¡œ RAG êµ¬ì¶• ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"RAG êµ¬ì¶• ì˜¤ë¥˜: {e}")
                    st.session_state.is_rag_ready = False
    else:
        st.session_state.is_rag_ready = False

    st.markdown("---")
    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        ["RAG ì§€ì‹ ì±—ë´‡", "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±", "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ"]
    )

st.title("âœ¨ ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜")

# ================================
# 5. ê¸°ëŠ¥ë³„ í˜ì´ì§€
# ================================
if feature_selection == "RAG ì§€ì‹ ì±—ë´‡":
    st.header("RAG ì±—ë´‡")
    if st.session_state.is_rag_ready:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        if prompt := st.chat_input("ì§ˆë¬¸ ì…ë ¥"):
            st.session_state.messages.append({"role":"user","content":prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.conversation_chain.invoke({"question": prompt})
                        answer = response.get("answer", "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
                        st.markdown(answer)
                        st.session_state.messages.append({"role":"assistant","content":answer})
                    except Exception as e:
                        st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}")
                        st.session_state.messages.append({"role":"assistant","content":"ì˜¤ë¥˜ ë°œìƒ"})
    else:
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ ìë£Œë¥¼ ì—…ë¡œë“œ í›„ ë¶„ì„ ì‹œì‘ ë²„íŠ¼ í´ë¦­")

elif feature_selection == "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±":
    st.header("ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±")
    topic = st.text_input("ì£¼ì œ ì…ë ¥")
    level = st.selectbox("ë‚œì´ë„", ["ì´ˆê¸‰","ì¤‘ê¸‰","ê³ ê¸‰"])
    content_type = st.selectbox("ì½˜í…ì¸  í˜•ì‹", ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸","í€´ì¦ˆ 3ë¬¸í•­","ì‹¤ìŠµ ì˜ˆì œ"])

    if st.button("ì½˜í…ì¸  ìƒì„±"):
        if topic:
            system_prompt = f"ë‹¹ì‹ ì€ {level} ìˆ˜ì¤€ì˜ ì „ë¬¸ AI ì½”ì¹˜ì…ë‹ˆë‹¤. ìš”ì²­í•œ ì£¼ì œì— ëŒ€í•´ {content_type} ìƒì„±. í•œêµ­ì–´ë¡œ."
            user_prompt = f"ì£¼ì œ: {topic}. í˜•ì‹: {content_type}."
            with st.spinner("ìƒì„± ì¤‘..."):
                try:
                    response = st.session_state.llm.invoke(user_prompt, system_instruction=system_prompt)
                    st.markdown(response.content)
                except Exception as e:
                    st.error(f"ìƒì„± ì˜¤ë¥˜: {e}")
        else:
            st.warning("ì£¼ì œ ì…ë ¥ í•„ìš”")

elif feature_selection == "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ":
    st.header("LSTM í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡")
    with st.spinner("ëª¨ë¸ ì¤€ë¹„ ì¤‘..."):
        try:
            lstm_model, historical_scores = load_or_train_lstm()
            st.success("ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

            look_back = 5
            input_seq = np.reshape(historical_scores[-look_back:], (1, look_back, 1))

            future_preds = []
            current_input = input_seq
            for _ in range(5):
                next_score = lstm_model.predict(current_input, verbose=0)[0]
                future_preds.append(next_score[0])
                current_input = np.append(current_input[:,1:,:], next_score[0]).reshape(1, look_back, 1)

            fig, ax = plt.subplots(figsize=(10,6))
            ax.plot(range(len(historical_scores)), historical_scores, label="ê³¼ê±° ì ìˆ˜", marker='o')
            ax.plot(range(len(historical_scores), len(historical_scores)+5), future_preds, label="ì˜ˆì¸¡ ì ìˆ˜", marker='x', linestyle='--', color='red')
            ax.set_title("LSTM ì„±ì·¨ë„ ì˜ˆì¸¡")
            ax.set_xlabel("ì£¼ê¸°")
            ax.set_ylabel("ì ìˆ˜")
            ax.legend()
            ax.grid(True)
            st.pyplot(fig)

            avg_recent = np.mean(historical_scores[-5:])
            avg_predict = np.mean(future_preds)

            if avg_predict > avg_recent:
                comment = "ì•ìœ¼ë¡œ ì„±ì·¨ë„ê°€ í–¥ìƒë  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤."
            elif avg_predict < avg_recent-5:
                comment = "ì„±ì·¨ë„ê°€ ë‹¤ì†Œ í•˜ë½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            else:
                comment = "ì„±ì·¨ë„ëŠ” í˜„ì¬ ìˆ˜ì¤€ì„ ìœ ì§€í•  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤."

            st.info(comment)

        except Exception as e:
            st.error(f"LSTM ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
