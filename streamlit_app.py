# ========================================
# Streamlit AI í•™ìŠµ ì½”ì¹˜ (RAG ìµœì¢… ìˆ˜ì •)
# ========================================
import streamlit as st
import os
import tempfile
import time
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, TextLoader
# [â­ì‚­ì œ] UnstructuredHTMLLoaderëŠ” NLTK ì˜ì¡´ì„± ë¬¸ì œë¡œ ì‚­ì œí•©ë‹ˆë‹¤.
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
# [â­ì¶”ê°€] í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸ ëˆ„ë½ ìˆ˜ì •
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup 

# ================================
# 1. LLM ë° ì„ë² ë”© ì´ˆê¸°í™” + ì„ë² ë”© ìºì‹œ
# ================================
API_KEY = os.environ.get("GEMINI_API_KEY")

if 'llm' not in st.session_state: # 'client' ëŒ€ì‹  'llm' ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    if not API_KEY: # API_KEYê°€ ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ Noneì¸ ê²½ìš°
        st.error("âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.session_state.is_llm_ready = False
    else:
        try:
            st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, google_api_key=API_KEY)
            st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=API_KEY)
            st.session_state.is_llm_ready = True
        except Exception as e:
            st.error(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”. {e}")
            st.session_state.is_llm_ready = False

# LangChain ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# [â­ì¶”ê°€â­] ì„¸ì…˜ ì„ë² ë”© ìºì‹œ ì´ˆê¸°í™”
if "embedding_cache" not in st.session_state:
    st.session_state.embedding_cache = {}

# --- RAG ê´€ë ¨ í•¨ìˆ˜ ---
def get_document_chunks(files):
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•©ë‹ˆë‹¤."""
    documents = []
    temp_dir = tempfile.mkdtemp()

    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        file_extension = uploaded_file.name.split('.')[-1].lower()
        
        # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ ë¡œë” ì„ íƒ (BeautifulSoup ì‚¬ìš© ë¡œì§ ìœ ì§€)
        if file_extension == "pdf":
            with open(temp_filepath, "wb") as f:
                f.write(uploaded_file.getvalue())
            loader = PyPDFLoader(temp_filepath)
            documents.extend(loader.load())
        
        elif file_extension == "html":
            # BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
            raw_html = uploaded_file.getvalue().decode('utf-8')
            soup = BeautifulSoup(raw_html, 'html.parser')
            text_content = soup.get_text(separator=' ', strip=True)
            
            # LangChain Document ê°ì²´ë¡œ ë³€í™˜
            from langchain.schema.document import Document
            documents.append(Document(page_content=text_content, metadata={"source": uploaded_file.name}))


        elif file_extension == "txt": # TXT íŒŒì¼ ì²˜ë¦¬
            with open(temp_filepath, "wb") as f:
                f.write(uploaded_file.getvalue())
            loader = TextLoader(temp_filepath, encoding="utf-8")
            documents.extend(loader.load())
            
        else:
            st.warning(f"'{uploaded_file.name}' íŒŒì¼ì€ í˜„ì¬ PDF, TXT, HTMLë§Œ ì§€ì›í•˜ì—¬ ë¡œë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

    # í…ìŠ¤íŠ¸ ë¶„í•  (ì²­í‚¹)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    
    # TextLoader/PyPDFLoaderì˜ ê²°ê³¼ëŠ” ì´ë¯¸ Document ê°ì²´ì´ë¯€ë¡œ ë°”ë¡œ split
    return text_splitter.split_documents(documents)


def get_vector_store(text_chunks):
    """í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  Vector Storeë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # [â­í•µì‹¬ ìˆ˜ì •â­] ë¬¸ì„œ ë‚´ìš©ì˜ í•´ì‹œê°’ì„ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ìºì‹œë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    cache_key = tuple(doc.page_content for doc in text_chunks)
    if cache_key in st.session_state.embedding_cache:
        st.info("âœ… ì„ë² ë”© ìºì‹œê°€ ë°œê²¬ë˜ì–´ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤. (API í•œë„ ì ˆì•½)")
        return st.session_state.embedding_cache[cache_key]
    
    if not st.session_state.is_llm_ready:
        return None

    try:
        vector_store = FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)
        st.session_state.embedding_cache[cache_key] = vector_store
        return vector_store
    
    except Exception as e:
        # 429 ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì‚¬ìš©ìì—ê²Œ ì •í™•í•˜ê²Œ ì•ˆë‚´
        if "429" in str(e):
             st.error("âš ï¸ **API ì„ë² ë”© í•œë„ ì´ˆê³¼ (429 Error)**: Google Gemini APIì˜ ë¬´ë£Œ ì„ë² ë”© ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ API ì‚¬ìš©ëŸ‰ ëŒ€ì‹œë³´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            st.error(f"Vector Store ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def get_rag_chain(vector_store):
    if vector_store is None:
        return None
        
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )

# ================================
# 4. Streamlit UI
# ================================
st.set_page_config(page_title="ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜", layout="wide")

with st.sidebar:
    st.title("ğŸ“š AI Study Coach ì„¤ì •")
    st.markdown("---")

    uploaded_files = st.file_uploader(
        "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        type=["pdf","txt","html"],
        accept_multiple_files=True
    )

    if uploaded_files and st.session_state.is_llm_ready:
        if st.button("ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)", key="start_analysis"):
            with st.spinner("ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘..."):
                text_chunks = get_document_chunks(uploaded_files)
                vector_store = get_vector_store(text_chunks)
                
                if vector_store:
                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                    st.success(f"ì´ {len(text_chunks)}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!")
                else:
                    st.session_state.is_rag_ready = False
                    st.error("ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ.")

    else:
        st.session_state.is_rag_ready = False
        st.warning("ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    st.markdown("---")
    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        ["RAG ì§€ì‹ ì±—ë´‡", "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±", "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ"]
    )

st.title("âœ¨ ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜")

# ================================
# 5. ê¸°ëŠ¥ë³„ í˜ì´ì§€ êµ¬í˜„
# ================================
if feature_selection == "RAG ì§€ì‹ ì±—ë´‡":
    st.header("RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)")
    st.markdown("ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")
    if st.session_state.is_rag_ready and st.session_state.conversation_chain:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”"):
            st.session_state.messages.append({"role":"user","content":prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.conversation_chain.invoke({"question":prompt})
                        answer = response.get('answer','ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role":"assistant","content":answer})
                    except Exception as e:
                        st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}")
                        st.session_state.messages.append({"role":"assistant","content":"ì˜¤ë¥˜ ë°œìƒ"})
    else:
        st.warning("RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”.")

# ================================
# ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±
# ================================
elif feature_selection == "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±":
    st.header("ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±")
    st.markdown("í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±")

    if st.session_state.is_llm_ready:
        topic = st.text_input("í•™ìŠµ ì£¼ì œ")
        level = st.selectbox("ë‚œì´ë„", ["ì´ˆê¸‰","ì¤‘ê¸‰","ê³ ê¸‰"])
        content_type = st.selectbox("ì½˜í…ì¸  í˜•ì‹", ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸","ê°ê´€ì‹ í€´ì¦ˆ 3ë¬¸í•­","ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"])

        if st.button("ì½˜í…ì¸  ìƒì„±"):
            if topic:
                # [â­ìˆ˜ì • ë¡œì§â­] system_instruction ëŒ€ì‹  í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•©í•˜ì—¬ LLMì— ì „ë‹¬
                full_prompt = f"""ë‹¹ì‹ ì€ {level} ìˆ˜ì¤€ì˜ ì „ë¬¸ AI ì½”ì¹˜ì…ë‹ˆë‹¤.
ìš”ì²­ë°›ì€ ì£¼ì œì— ëŒ€í•´ {content_type} í˜•ì‹ì— ë§ì¶° ëª…í™•í•˜ê³  êµìœ¡ì ì¸ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”. ë‹µë³€ì€ í•œêµ­ì–´ë¡œë§Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ì£¼ì œ: {topic}
ìš”ì²­ í˜•ì‹: {content_type}"""

                with st.spinner(f"{topic}ì— ëŒ€í•œ {content_type} ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.llm.invoke(full_prompt) # system_instruction ì¸ìˆ˜ë¥¼ ì œê±°
                        st.success(f"**{topic}** - **{content_type}** ê²°ê³¼:")
                        st.markdown(response.content)
                    except Exception as e:
                        st.error(f"ì½˜í…ì¸  ìƒì„± ì˜¤ë¥˜: {e}")
            else:
                st.warning("í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        st.error("LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")

# ================================
# LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ (ë¹„í™œì„±í™”)
# ================================
elif feature_selection == "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ":
    st.header("LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ")
    st.markdown("LSTM ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ TensorFlow ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì„¤ì¹˜ê°€ ì„ í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
    st.error("í˜„ì¬ ë¹Œë“œ í™˜ê²½ ë¬¸ì œë¡œ ì¸í•´ LSTM ê¸°ëŠ¥ì€ ì ì •ì ìœ¼ë¡œ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. 'ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±' ê¸°ëŠ¥ì„ ë¨¼ì € ì‚¬ìš©í•´ ì£¼ì„¸ìš”.")
