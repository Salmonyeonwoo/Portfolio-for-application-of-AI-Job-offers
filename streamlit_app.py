# ========================================
# Streamlit AI í•™ìŠµ ì½”ì¹˜ (RAG ìµœì¢… ìˆ˜ì •)
# ========================================
import streamlit as st
import os
import tempfile
import time
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, TextLoader
# [â­ì‚­ì œ] UnstructuredHTMLLoaderëŠ” NLTK ì˜ì¡´ì„± ë¬¸ì œë¡œ ì‚­ì œí•©ë‹ˆë‹¤.
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup # [â­ì¶”ê°€] HTML íŒŒì‹±ì„ ìœ„í•´ BeautifulSoup4 ì¶”ê°€

# [â­ì‚­ì œ] NLTK ì˜ì¡´ì„± ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ ë° ì„í¬íŠ¸ë¥¼ ëª¨ë‘ ì œê±°í•©ë‹ˆë‹¤.
# NLTK ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì‚­ì œ

# ================================
# 1. LLM ë° ì„ë² ë”© ì´ˆê¸°í™” + ì„ë² ë”© ìºì‹œ
# (ì´ì „ ì½”ë“œì™€ ë™ì¼, LLM ì´ˆê¸°í™” ë¡œì§ ìœ ì§€)
# ================================
API_KEY = os.environ.get("GEMINI_API_KEY")

if 'client' not in st.session_state:
    if not API_KEY: # API_KEYê°€ ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ Noneì¸ ê²½ìš°
        st.error("âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.session_state.is_llm_ready = False
    else:
        try:
            st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, google_api_key=API_KEY)
            st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=API_KEY)
            st.session_state.is_llm_ready = True
        except Exception as e:
            st.error(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”. {e}")
            st.session_state.is_llm_ready = False

# LangChain ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)


# --- RAG ê´€ë ¨ í•¨ìˆ˜ ---
def get_document_chunks(files):
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•©ë‹ˆë‹¤."""
    documents = []
    temp_dir = tempfile.mkdtemp()

    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        file_extension = uploaded_file.name.split('.')[-1].lower()
        
        # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ ë¡œë” ì„ íƒ
        if file_extension == "pdf":
            # PDFëŠ” PyPDFLoader ì‚¬ìš©
            with open(temp_filepath, "wb") as f:
                f.write(uploaded_file.getvalue())
            loader = PyPDFLoader(temp_filepath)
            documents.extend(loader.load())
        
        elif file_extension == "html":
            # [â­í•µì‹¬] BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
            raw_html = uploaded_file.getvalue().decode('utf-8')
            soup = BeautifulSoup(raw_html, 'html.parser')
            text_content = soup.get_text(separator=' ', strip=True)
            
            # LangChain TextLoaderì˜ ë¬¸ì„œ í˜•íƒœë¡œ ë³€í™˜
            documents.append({"page_content": text_content, "metadata": {"source": uploaded_file.name}})

        else: # TXT íŒŒì¼ ì²˜ë¦¬
            # TextLoader ì‚¬ìš©
            with open(temp_filepath, "wb") as f:
                f.write(uploaded_file.getvalue())
            loader = TextLoader(temp_filepath, encoding="utf-8")
            documents.extend(loader.load())

    # í…ìŠ¤íŠ¸ ë¶„í•  (ì²­í‚¹)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    # [ìˆ˜ì •] HTML ì²˜ë¦¬ ì‹œ ë¬¸ì„œ í˜•íƒœê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, List[Document] í˜•íƒœë¡œ ëª…ì‹œì  ë³€í™˜ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì•ˆì „í•˜ê²Œ LangChainì˜ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ì§€ë§Œ, 
    # í˜„ì¬ ì½”ë“œ êµ¬ì¡°ì—ì„œëŠ” TextLoaderì™€ PyPDFLoaderì˜ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    # HTML ë¡œë“œ ê²°ê³¼ê°€ Dictì¸ ê²½ìš° Document ê°ì²´ë¡œ ë³€í™˜
    final_documents = []
    for doc in documents:
        if isinstance(doc, dict) and 'page_content' in doc:
            from langchain.schema.document import Document
            final_documents.append(Document(page_content=doc['page_content'], metadata=doc.get('metadata', {})))
        else:
            final_documents.append(doc)
            
    return text_splitter.split_documents(final_documents)


def get_vector_store(text_chunks):
    # (ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€)
    try:
        vector_store = FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)
        return vector_store
    except ImportError:
        st.error("FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. requirements.txtì— 'faiss-cpu'ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        return None
    except Exception as e:
        st.error(f"Vector Store ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def get_rag_chain(vector_store):
    # (ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€)
    if vector_store is None:
        return None
        
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )

# ================================
# 4. Streamlit UI
# ================================
st.set_page_config(page_title="ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜", layout="wide")

with st.sidebar:
    st.title("ğŸ“š AI Study Coach ì„¤ì •")
    st.markdown("---")

    uploaded_files = st.file_uploader(
        "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        type=["pdf","txt","html"],
        accept_multiple_files=True
    )

    if uploaded_files and st.session_state.is_llm_ready:
        if st.button("ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)", key="start_analysis"):
            with st.spinner("ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘..."):
                text_chunks = get_document_chunks(uploaded_files)
                vector_store = get_vector_store(text_chunks)
                if vector_store:
                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                    st.success(f"ì´ {len(text_chunks)}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!")
                else:
                    st.session_state.is_rag_ready = False
                    st.error("ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ.")

    else:
        st.session_state.is_rag_ready = False
        st.warning("ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    st.markdown("---")
    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        ["RAG ì§€ì‹ ì±—ë´‡", "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±", "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ"]
    )

st.title("âœ¨ ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜")

# ================================
# 5. ê¸°ëŠ¥ë³„ í˜ì´ì§€ êµ¬í˜„
# ================================
if feature_selection == "RAG ì§€ì‹ ì±—ë´‡":
    st.header("RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)")
    st.markdown("ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")
    if st.session_state.is_rag_ready and st.session_state.conversation_chain:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”"):
            st.session_state.messages.append({"role":"user","content":prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.conversation_chain.invoke({"question":prompt})
                        answer = response.get('answer','ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role":"assistant","content":answer})
                    except Exception as e:
                        st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}")
                        st.session_state.messages.append({"role":"assistant","content":"ì˜¤ë¥˜ ë°œìƒ"})
    else:
        st.warning("RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”.")

# ================================
# ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±
# ================================
elif feature_selection == "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±":
    st.header("ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±")
    st.markdown("í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±")

    if st.session_state.is_llm_ready:
        topic = st.text_input("í•™ìŠµ ì£¼ì œ")
        level = st.selectbox("ë‚œì´ë„", ["ì´ˆê¸‰","ì¤‘ê¸‰","ê³ ê¸‰"])
        content_type = st.selectbox("ì½˜í…ì¸  í˜•ì‹", ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸","ê°ê´€ì‹ í€´ì¦ˆ 3ë¬¸í•­","ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"])

        if st.button("ì½˜í…ì¸  ìƒì„±"):
            if topic:
                system_prompt = f"""ë‹¹ì‹ ì€ {level} ìˆ˜ì¤€ì˜ ì „ë¬¸ AI ì½”ì¹˜ì…ë‹ˆë‹¤.
ìš”ì²­ë°›ì€ ì£¼ì œì— ëŒ€í•´ {content_type} í˜•ì‹ì— ë§ì¶° ëª…í™•í•˜ê³  êµìœ¡ì ì¸ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œë§Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤."""

                user_prompt = f"ì£¼ì œ: {topic}. ìš”ì²­ í˜•ì‹: {content_type}."

                with st.spinner(f"{topic}ì— ëŒ€í•œ {content_type} ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.llm.invoke(
                            user_prompt,
                            system_instruction=system_prompt
                        )
                        st.success(f"**{topic}** - **{content_type}** ê²°ê³¼:")
                        st.markdown(response.content)
                    except Exception as e:
                        st.error(f"ì½˜í…ì¸  ìƒì„± ì˜¤ë¥˜: {e}")
            else:
                st.warning("í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")



