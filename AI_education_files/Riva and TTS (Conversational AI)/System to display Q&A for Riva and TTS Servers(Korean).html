<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Language Model Evolution and TTS Feature Demo</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            background-color: #f8fafc; /* Light Blue/Gray Background */
        }
        .slide {
            background-color: white;
            padding: 2rem;
            border-radius: 0.75rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            margin-bottom: 2rem;
        }
        .slide-title {
            font-size: 1.75rem;
            font-weight: 700;
            color: #10b981;
            border-bottom: 2px solid #d1fae5;
            padding-bottom: 0.5rem;
            margin-bottom: 1rem;
        }
        .loader {
            border: 4px solid #f3f3f3;
            border-top: 4px solid #3498db;
            border-radius: 50%;
            width: 24px;
            height: 24px;
            animation: spin 1s linear infinite;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        .audio-container {
            display: flex;
            align-items: center;
            gap: 1rem;
            margin-top: 1rem;
            flex-wrap: wrap;
        }

        /* Evolution Flow Chart Styles */
        .flow-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            gap: 1rem;
            flex-wrap: wrap; /* Mobile Responsiveness */
        }
        .flow-step {
            flex: 1 1 200px; /* Flexibility */
            padding: 1.5rem;
            border-radius: 0.5rem;
            text-align: center;
            position: relative;
            background-color: #d1fae5;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .flow-step h3 {
            font-weight: 700;
            color: #047857; /* Emerald 700 */
            margin-bottom: 0.25rem;
        }
        .flow-step p {
            font-size: 0.875rem;
            color: #065f46; /* Emerald 800 */
        }
        .arrow-icon {
            color: #10b981;
            font-size: 2rem;
            font-weight: bold;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 0 0.5rem;
        }

        @media (max-width: 768px) {
            .flow-container {
                flex-direction: column;
            }
            .flow-step {
                width: 100%;
                flex: none;
            }
            .arrow-icon {
                transform: rotate(90deg);
                margin: 0.5rem 0;
            }
        }
    </style>
</head>
<body class="bg-gray-100">
    <div class="container mx-auto p-4 md:p-8 max-w-4xl">
        <header class="text-center mb-10">
            <h1 class="text-4xl font-bold text-gray-800">AI Language Model Evolution and TTS Feature Demo</h1>
            <p class="text-lg text-gray-600 mt-2">Evolution from Seq2Seq to LLM and Text-to-Speech (TTS) Feature Demonstration</p>
        </header>

        <!-- Slide 1: AI Language Model Evolution (Visualization) -->
        <div class="slide">
            <h2 class="slide-title"><i class="fas fa-history mr-2"></i> 1. AI Language Model Evolution Flowchart</h2>
            
            <div class="flow-container">
                
                <!-- Step 1: Seq2Seq (2014) -->
                <div class="flow-step">
                    <h3 class="text-xl">Seq2Seq (2014)</h3>
                    <p>RNN-based Encoder-Decoder structure.</p>
                    <p class="font-bold">Limitation: Difficulty in long-term context understanding.</p>
                </div>

                <!-- Arrow 1 -->
                <div class="arrow-icon">â†’</div>

                <!-- Step 2: Attention (2015) -->
                <div class="flow-step bg-green-200">
                    <h3 class="text-xl">Attention Mechanism (2015)</h3>
                    <p>Decoder focuses on specific parts of the input to improve accuracy.</p>
                    <p class="font-bold">Improvement: Partially resolved long-term dependency issues.</p>
                </div>

                <!-- Arrow 2 -->
                <div class="arrow-icon">â†’</div>
                
                <!-- Step 3: Transformer (2017) -->
                <div class="flow-step bg-green-300">
                    <h3 class="text-xl">Transformer (2017)</h3>
                    <p>Uses only Attention to maximize parallel processing.</p>
                    <p class="font-bold text-red-700">Key: Watershed moment for AI model development.</p>
                </div>

                <!-- Arrow 3 -->
                <div class="arrow-icon">â†’</div>

                <!-- Step 4: GPT & LLM (Current) -->
                <div class="flow-step bg-green-400">
                    <h3 class="text-xl">GPT & LLM (Current)</h3>
                    <p>Transformer-based large-scale language models.</p>
                    <p class="font-bold">Result: Achieved general-purpose language capabilities.</p>
                </div>

            </div>

            <div class="mt-8 text-sm text-gray-700">
                <p>ðŸ’¡ **Importance of Transformer:** It eliminated RNN/CNNs, revolutionizing training speed and establishing the foundation for Large Language Model (LLM) development.</p>
            </div>
        </div>

        <!-- Slide 2: TTS (Text-to-Speech) Feature Demo -->
        <div class="slide">
            <h2 class="slide-title"><i class="fas fa-volume-up mr-2"></i> 2. Text-to-Speech (TTS) Feature Demo</h2>
            <p class="text-gray-700 mb-4">Demonstrating the Gemini API's TTS feature to convert text to speech in various situations and voices.</p>

            <div id="error-message" class="bg-red-100 border border-red-400 text-red-700 px-4 py-3 rounded relative hidden mb-4" role="alert">
                <strong class="font-bold">Error!</strong>
                <span class="block sm:inline"> An error occurred during audio generation. (Check API Key)</span>
            </div>

            <div class="space-y-6">
                <!-- TTS Example 1: Informative (Charon) -->
                <div class="p-4 border rounded-lg bg-gray-50">
                    <p class="font-semibold text-gray-800">Example 1: Information Delivery (Informative)</p>
                    <p class="text-sm text-gray-600 italic">Spoken in the "Charon" voice with an informative tone.</p>
                    <div class="audio-container">
                        <button data-text="Charon voice, informative tone: Our AI model has analyzed the current market situation and identified three key investment opportunities." data-voice="Charon" class="play-button bg-blue-600 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded-full inline-flex items-center transition duration-300">
                            <i class="fas fa-play mr-2"></i> Listen (EN)
                        </button>
                        <div class="download-container hidden"></div>
                    </div>
                </div>

                <!-- TTS Example 2: Cheerful (Puck) -->
                <div class="p-4 border rounded-lg bg-gray-50">
                    <p class="font-semibold text-gray-800">Example 2: Lively Announcement (Upbeat)</p>
                    <p class="text-sm text-gray-600 italic">Spoken in the "Puck" voice with an upbeat tone.</p>
                    <div class="audio-container">
                        <button data-text="Puck voice, upbeat tone: New AI features have been updated! Connect now and experience our faster, smarter service!" data-voice="Puck" class="play-button bg-blue-600 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded-full inline-flex items-center transition duration-300">
                            <i class="fas fa-play mr-2"></i> Listen (EN)
                        </button>
                        <div class="download-container hidden"></div>
                    </div>
                </div>

                <!-- TTS Example 3: Multi-Speaker Conversation (KR) -->
                <div class="p-4 border rounded-lg bg-gray-50">
                    <p class="font-semibold text-gray-800">Example 3: Multi-Speaker Conversation (Korean)</p>
                    <p class="text-sm text-gray-600 italic">Conversation spoken by "Joe" and "Jane" voices in Korean.</p>
                    <div class="audio-container">
                        <button data-text="TTS the following conversation between Joe and Jane in Korean:\nJoe: ì´ë²ˆ ì£¼ AI í”„ë¡œì íŠ¸ì˜ QA ê²€í† ëŠ” ìž˜ ì§„í–‰ë˜ê³  ìžˆë‚˜ìš”?\nJane: ë„¤, í•µì‹¬ ë²„ê·¸ëŠ” ëª¨ë‘ ìˆ˜ì •ë˜ì—ˆê³ , ì‚¬ìš©ìž ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ë§Œ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ë‚´ì¼ ìµœì¢… ë³´ê³  ë“œë¦´ê²Œìš”." data-voice-config='{"speakerVoiceConfigs": [{"speaker": "Joe", "voiceConfig": {"prebuiltVoiceConfig": {"voiceName": "Kore"}}}, {"speaker": "Jane", "voiceConfig": {"prebuiltVoiceConfig": {"voiceName": "Leda"}}}]}' class="play-button bg-blue-600 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded-full inline-flex items-center transition duration-300">
                            <i class="fas fa-play mr-2"></i> Listen (Conversation)
                        </button>
                        <div class="download-container hidden"></div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const playButtons = document.querySelectorAll('.play-button');
            const errorMessage = document.getElementById('error-message');
            const apiKey = ""; // API key is injected in the Canvas environment.
            const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent?key=${apiKey}`;

            // Base64 to ArrayBuffer helper function
            const base64ToArrayBuffer = (base64) => {
                const binaryString = atob(base64);
                const len = binaryString.length;
                const bytes = new Uint8Array(len);
                for (let i = 0; i < len; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }
                return bytes.buffer;
            };

            // PCM to WAV conversion helper function
            const pcmToWav = (pcm16, sampleRate = 24000) => {
                const numChannels = 1;
                const pcmData = pcm16.buffer;
                const buffer = new ArrayBuffer(44 + pcmData.byteLength);
                const view = new DataView(buffer);
                let offset = 0;

                // Write string to view
                const writeString = (view, offset, str) => {
                    for (let i = 0; i < str.length; i++) {
                        view.setUint8(offset + i, str.charCodeAt(i));
                    }
                };

                // RIFF chunk
                writeString(view, offset, 'RIFF'); offset += 4;
                view.setUint32(offset, 36 + pcmData.byteLength, true); offset += 4;
                writeString(view, offset, 'WAVE'); offset += 4;

                // FMT chunk
                writeString(view, offset, 'fmt '); offset += 4;
                view.setUint32(offset, 16, true); offset += 4; // Sub-chunk size
                view.setUint16(offset, 1, true); offset += 2; // Audio format (1 for PCM)
                view.setUint16(offset, numChannels, true); offset += 2; // Number of channels
                view.setUint32(offset, sampleRate, true); offset += 4; // Sample rate
                view.setUint32(offset, sampleRate * numChannels * 2, true); offset += 4; // Byte rate
                view.setUint16(offset, numChannels * 2, true); offset += 2; // Block align
                view.setUint16(offset, 16, true); offset += 2; // Bits per sample

                // Data chunk
                writeString(view, offset, 'data'); offset += 4;
                view.setUint32(offset, pcmData.byteLength, true); offset += 4; // Data size

                // Write PCM data
                const pcmBytes = new Uint8Array(pcmData);
                for (let i = 0; i < pcmData.byteLength; i++) {
                    view.setUint8(offset + i, pcmBytes[i]);
                }

                return new Blob([view], { type: 'audio/wav' });
            };

            const generateAndPlayAudio = async (button) => {
                const originalContent = button.innerHTML;
                button.disabled = true;
                button.innerHTML = '<span class="loader"></span><span class="ml-2">Generating...</span>';
                errorMessage.classList.add('hidden');

                const text = button.getAttribute('data-text');
                const voiceName = button.getAttribute('data-voice');
                const voiceConfigJson = button.getAttribute('data-voice-config');
                const downloadContainer = button.parentNode.querySelector('.download-container');

                // Clear previous download link
                downloadContainer.innerHTML = '';
                downloadContainer.style.display = 'none';

                let generationConfig = { responseModalities: ["AUDIO"] };
                
                if (voiceConfigJson) {
                    // Multi-speaker configuration
                    generationConfig = { 
                        ...generationConfig,
                        ...JSON.parse(voiceConfigJson)
                    };
                } else if (voiceName) {
                    // Single-speaker configuration
                    generationConfig = {
                        ...generationConfig,
                        speechConfig: {
                            voiceConfig: {
                                prebuiltVoiceConfig: { voiceName: voiceName }
                            }
                        }
                    };
                }

                const payload = {
                    contents: [{ parts: [{ text: text }] }],
                    generationConfig: generationConfig,
                    model: "gemini-2.5-flash-preview-tts"
                };

                try {
                    // Implement exponential backoff for API call
                    let response;
                    for (let attempt = 0; attempt < 3; attempt++) {
                        response = await fetch(apiUrl, {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' },
                            body: JSON.stringify(payload)
                        });

                        if (response.status !== 429) {
                            break; // Success or non-rate-limit error
                        }

                        // Exponential backoff
                        const delay = Math.pow(2, attempt) * 1000;
                        await new Promise(resolve => setTimeout(resolve, delay));
                    }
                    
                    if (!response.ok) {
                         // Throw detailed error for non-rate-limit failures
                         const errorBody = await response.json();
                         throw new Error(`API Error: ${response.status} - ${errorBody.error?.message || 'Unknown error'}`);
                    }

                    const result = await response.json();
                    const part = result?.candidates?.[0]?.content?.parts?.[0];
                    const audioData = part?.inlineData?.data;
                    const mimeType = part?.inlineData?.mimeType;

                    if (audioData && mimeType && mimeType.startsWith("audio/L16")) {
                        // API returns raw signed PCM 16 bit audio data (L16)
                        const pcmDataBuffer = base64ToArrayBuffer(audioData);
                        const pcm16 = new Int16Array(pcmDataBuffer);
                        
                        // Extract sample rate (defaults to 24000 if not explicitly in mimetype)
                        let sampleRate = 24000;
                        const rateMatch = mimeType.match(/rate=(\d+)/);
                        if (rateMatch && rateMatch[1]) {
                            sampleRate = parseInt(rateMatch[1], 10);
                        }
                        
                        // Convert PCM to WAV Blob
                        const wavBlob = pcmToWav(pcm16, sampleRate);
                        const audioUrl = URL.createObjectURL(wavBlob);

                        // Play audio
                        const audio = new Audio(audioUrl);
                        audio.play();

                        // Add download button
                        const filename = 'tts_audio.wav';
                        const downloadLink = document.createElement('a');
                        downloadLink.href = audioUrl;
                        downloadLink.download = filename;
                        downloadLink.className = 'bg-green-600 hover:bg-green-700 text-white font-bold py-2 px-4 rounded-full inline-flex items-center transition duration-300';
                        downloadLink.innerHTML = `<i class="fas fa-download mr-2"></i> Download .wav`;
                        downloadContainer.appendChild(downloadLink);
                        downloadContainer.style.display = 'block';

                    } else {
                        throw new Error("Invalid audio data in response or missing data.");
                    }

                } catch (error) {
                    console.error('Error generating audio:', error);
                    errorMessage.querySelector('span').textContent = ` An error occurred during audio generation: ${error.message}`;
                    errorMessage.classList.remove('hidden');
                } finally {
                    button.disabled = false;
                    button.innerHTML = originalContent;
                }
            };

            playButtons.forEach(button => {
                button.addEventListener('click', () => generateAndPlayAudio(button));
            });
        });
    </script>
</body>
</html>
